{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TempTrimmed_Norm400Hz_Exp_030_AA.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_031_AA.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_032_AA.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_033_AA.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_035_AA.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_062_BB.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_065_BB.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_067_BB.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_068_BB.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_069_BB.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_066_CC.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_070_CC.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_071_CC.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_074_CC.csv Loaded\n",
      "Data Set Retrieved\n",
      "TempTrimmed_Norm400Hz_Exp_075_CC.csv Loaded\n",
      "Data Set Retrieved\n",
      "Data Loading Finished\n",
      "Data Length: 1703460\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712084376.011627   65903 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.9218 - accuracy: 0.5755\n",
      "Epoch 1: accuracy improved from -inf to 0.57550, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 20s 10ms/step - loss: 0.9217 - accuracy: 0.5755 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "  16/1664 [..............................] - ETA: 16s - loss: 0.7854 - accuracy: 0.6304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.7140 - accuracy: 0.6567\n",
      "Epoch 2: accuracy improved from 0.57550 to 0.65673, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 17s 10ms/step - loss: 0.7139 - accuracy: 0.6567 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.6397 - accuracy: 0.7000\n",
      "Epoch 3: accuracy improved from 0.65673 to 0.70002, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.6396 - accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7242\n",
      "Epoch 4: accuracy improved from 0.70002 to 0.72421, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.5995 - accuracy: 0.7242 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "1663/1664 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7350\n",
      "Epoch 5: accuracy improved from 0.72421 to 0.73505, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.5707 - accuracy: 0.7350 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7456\n",
      "Epoch 6: accuracy improved from 0.73505 to 0.74558, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.5460 - accuracy: 0.7456 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7595\n",
      "Epoch 7: accuracy improved from 0.74558 to 0.75950, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.5219 - accuracy: 0.7595 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.4959 - accuracy: 0.7768\n",
      "Epoch 8: accuracy improved from 0.75950 to 0.77687, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4958 - accuracy: 0.7769 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "1661/1664 [============================>.] - ETA: 0s - loss: 0.4736 - accuracy: 0.7917\n",
      "Epoch 9: accuracy improved from 0.77687 to 0.79171, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4736 - accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.4547 - accuracy: 0.8031\n",
      "Epoch 10: accuracy improved from 0.79171 to 0.80313, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4547 - accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "1663/1664 [============================>.] - ETA: 0s - loss: 0.4406 - accuracy: 0.8116\n",
      "Epoch 11: accuracy improved from 0.80313 to 0.81164, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4406 - accuracy: 0.8116 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.4254 - accuracy: 0.8209\n",
      "Epoch 12: accuracy improved from 0.81164 to 0.82089, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4254 - accuracy: 0.8209 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "1664/1664 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.8289\n",
      "Epoch 13: accuracy improved from 0.82089 to 0.82888, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.4121 - accuracy: 0.8289 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "1664/1664 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8369\n",
      "Epoch 14: accuracy improved from 0.82888 to 0.83686, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3975 - accuracy: 0.8369 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "1663/1664 [============================>.] - ETA: 0s - loss: 0.3838 - accuracy: 0.8443\n",
      "Epoch 15: accuracy improved from 0.83686 to 0.84435, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3838 - accuracy: 0.8443 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8510\n",
      "Epoch 16: accuracy improved from 0.84435 to 0.85100, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3713 - accuracy: 0.8510 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "1660/1664 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.8570\n",
      "Epoch 17: accuracy improved from 0.85100 to 0.85702, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 9ms/step - loss: 0.3600 - accuracy: 0.8570 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "1663/1664 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8623\n",
      "Epoch 18: accuracy improved from 0.85702 to 0.86232, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 9ms/step - loss: 0.3497 - accuracy: 0.8623 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "1662/1664 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8677\n",
      "Epoch 19: accuracy improved from 0.86232 to 0.86774, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3388 - accuracy: 0.8677 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.3299 - accuracy: 0.8723\n",
      "Epoch 20: accuracy improved from 0.86774 to 0.87235, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3299 - accuracy: 0.8724 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "1664/1664 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.8766\n",
      "Epoch 21: accuracy improved from 0.87235 to 0.87664, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3210 - accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "1663/1664 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8801\n",
      "Epoch 22: accuracy improved from 0.87664 to 0.88012, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3135 - accuracy: 0.8801 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "1661/1664 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8841\n",
      "Epoch 23: accuracy improved from 0.88012 to 0.88407, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.3052 - accuracy: 0.8841 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "1660/1664 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8871\n",
      "Epoch 24: accuracy improved from 0.88407 to 0.88715, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 17s 10ms/step - loss: 0.2982 - accuracy: 0.8871 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "1659/1664 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8903\n",
      "Epoch 25: accuracy improved from 0.88715 to 0.89037, saving model to CG_Model.h5\n",
      "1664/1664 [==============================] - 16s 10ms/step - loss: 0.2910 - accuracy: 0.8904 - lr: 0.0010\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, GRU, BatchNormalization, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from Q3_DataLoader import RNN_load_data, trim_data_to_min_file_length\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Length of the input sequence during training\n",
    "timesteps = 64\n",
    "data_coeff = 1\n",
    "num_bins = 3\n",
    "\n",
    "# Data Sets\n",
    "#REV 1\n",
    "dataSet1 = \"Norm400Hz_005_AA.csv\"\n",
    "dataSet2 = \"Norm400Hz_006_BB.csv\" #BAD DATASET - AltHold @ 4m\n",
    "dataSet3 = \"Norm400Hz_007_AA.csv\"\n",
    "dataSet4 = \"Norm400Hz_008_AA.csv\"\n",
    "dataSet5 = \"Norm400Hz_009_CC.csv\" #SHORT\n",
    "dataSet6 = \"Norm400Hz_010_AA.csv\"\n",
    "dataSet7 = \"Norm400Hz_014_AA.csv\" #BAD DATASET - No 3D display\n",
    "dataSet8 = \"Norm400Hz_015_AA.csv\"\n",
    "dataSet9 = \"Norm400Hz_016_BB.csv\" #SHORT\n",
    "dataSet10 = \"Norm400Hz_019_AA.csv\" #BAD DATASET\n",
    "dataSet11 = \"Norm400Hz_020_CC.csv\" #BAD DATASET - Flying at .3m? \n",
    "dataSet12 = \"Norm400Hz_021_CC.csv\"\n",
    "dataSet13 = \"Norm400Hz_022_AA.csv\"\n",
    "dataSet14 = \"Norm400Hz_023_AA.csv\"\n",
    "#REV 2\n",
    "dataSet15 = \"Norm400Hz_025_AA.csv\" #BAD DATASET - No 3D display\n",
    "dataSet16 = \"Norm400Hz_026_CC.csv\"\n",
    "dataSet17 = \"Norm400Hz_027_CC.csv\" #DROPOUT\n",
    "dataSet18 = \"Norm400Hz_028_AA.csv\" #SHORT\n",
    "#REV 3\n",
    "dataSet19 = \"Norm400Hz_029_AA.csv\" #BAD DATASET\n",
    "dataSet20 = \"Norm400Hz_030_AA.csv\"\n",
    "dataSet21 = \"Norm400Hz_031_AA.csv\"\n",
    "dataSet22 = \"Norm400Hz_032_AA.csv\"\n",
    "dataSet23 = \"Norm400Hz_033_AA.csv\"\n",
    "dataSet24 = \"Norm400Hz_034_AA.csv\"\n",
    "dataSet25 = \"Norm400Hz_035_AA.csv\"\n",
    "dataSet26 = \"Norm400Hz_049_AA.csv\"\n",
    "dataSet27 = \"Norm400Hz_051_AA.csv\"\n",
    "dataSet28 = \"Norm400Hz_053_AA.csv\"\n",
    "dataSet29 = \"Norm400Hz_054_AA.csv\"\n",
    "dataSet30 = \"Norm400Hz_055_AA.csv\"\n",
    "dataSet31 = \"Norm400Hz_056_AA.csv\"\n",
    "dataSet32 = \"Norm400Hz_057_AA.csv\"\n",
    "dataSet33 = \"Norm400Hz_058_AA.csv\"\n",
    "dataSet34 = \"Norm400Hz_059_AA.csv\"\n",
    "dataSet35 = \"Norm400Hz_060_AA.csv\"\n",
    "dataSet36 = \"Norm400Hz_061_AA.csv\"\n",
    "dataSet37 = \"Norm400Hz_062_BB.csv\"\n",
    "dataSet38 = \"Norm400Hz_065_BB.csv\"\n",
    "dataSet39 = \"Norm400Hz_066_CC.csv\"\n",
    "dataSet40 = \"Norm400Hz_067_BB.csv\" #Already trimmed to avoid dropout \n",
    "dataSet41 = \"Norm400Hz_068_BB.csv\"\n",
    "dataSet42 = \"Norm400Hz_069_BB.csv\"\n",
    "dataSet43 = \"Norm400Hz_070_CC.csv\"\n",
    "dataSet44 = \"Norm400Hz_071_CC.csv\"\n",
    "dataSet45 = \"Norm400Hz_072_BB.csv\"\n",
    "dataSet46 = \"Norm400Hz_073_BB.csv\"\n",
    "dataSet47 = \"Norm400Hz_074_CC.csv\"\n",
    "dataSet48 = \"Norm400Hz_075_CC.csv\"\n",
    "dataSet49 = \"Norm400Hz_076_CC.csv\"\n",
    "#DataSets with ATT, IMU_0, RCOU\n",
    "dataSet50 = \"Norm400Hz_Exp_030_AA.csv\"\n",
    "dataSet51 = \"Norm400Hz_Exp_031_AA.csv\"\n",
    "dataSet52 = \"Norm400Hz_Exp_032_AA.csv\"\n",
    "dataSet53 = \"Norm400Hz_Exp_033_AA.csv\"\n",
    "dataSet54 = \"Norm400Hz_Exp_035_AA.csv\"\n",
    "dataSet55 = \"Norm400Hz_Exp_051_AA.csv\"\n",
    "dataSet56 = \"Norm400Hz_Exp_054_AA.csv\"\n",
    "dataSet57 = \"Norm400Hz_Exp_056_AA.csv\"\n",
    "dataSet58 = \"Norm400Hz_Exp_062_BB.csv\"\n",
    "dataSet59 = \"Norm400Hz_Exp_065_BB.csv\"\n",
    "dataSet60 = \"Norm400Hz_Exp_066_CC.csv\"\n",
    "dataSet61 = \"Norm400Hz_Exp_067_BB.csv\"\n",
    "dataSet62 = \"Norm400Hz_Exp_068_BB.csv\"\n",
    "dataSet63 = \"Norm400Hz_Exp_069_BB.csv\"\n",
    "dataSet64 = \"Norm400Hz_Exp_070_CC.csv\"\n",
    "dataSet65 = \"Norm400Hz_Exp_071_CC.csv\"\n",
    "dataSet66 = \"Norm400Hz_Exp_072_BB.csv\"\n",
    "dataSet67 = \"Norm400Hz_Exp_073_BB.csv\"\n",
    "dataSet68 = \"Norm400Hz_Exp_074_CC.csv\"\n",
    "dataSet69 = \"Norm400Hz_Exp_075_CC.csv\"\n",
    "dataSet70 = \"Norm400Hz_Exp_076_CC.csv\"\n",
    "\n",
    "# # DataSets with only IMU_0, and RCOU\n",
    "# # Categorize the datasets. This list is used for training\n",
    "# datasets_AA = [dataSet20, dataSet21, dataSet22, dataSet23, dataSet24]#, dataSet25, dataSet26, dataSet27, dataSet28, dataSet29, dataSet30, dataSet31, dataSet32]#, dataSet33, dataSet34, dataSet35, dataSet36]\n",
    "# datasets_BB = [dataSet37, dataSet38, dataSet40, dataSet41, dataSet42]#, dataSet45]#, dataSet46]\n",
    "# datasets_CC = [dataSet39, dataSet43, dataSet44, dataSet47, dataSet48]#, dataSet49]\n",
    "\n",
    "# # Do not train on this data. Use it for predictions\n",
    "# predict_AA = [dataSet33, dataSet34, dataSet35, dataSet36]\n",
    "# predict_BB = [dataSet46]#, dataSet22, dataSet30]\n",
    "# predict_CC = [dataSet49]#, dataSet16, dataSet32]\n",
    "\n",
    "# DataSets with ATT, IMU_0, and RCOU\n",
    "datasets_AA = [dataSet50, dataSet51, dataSet52, dataSet53, dataSet54]#, dataSet55, dataSet56, dataSet57]\n",
    "datasets_BB = [dataSet58, dataSet59, dataSet61, dataSet62, dataSet63]#, dataSet66, dataSet67]\n",
    "datasets_CC = [dataSet60, dataSet64, dataSet65, dataSet68, dataSet69]#, dataSet70]\n",
    "datasets_DD = []\n",
    "datasets_EE = []\n",
    "\n",
    "# Shuffle the datasets in each category so that the model does not always train off of the earliest flights\n",
    "# random.shuffle(datasets_AA)\n",
    "# random.shuffle(datasets_BB)\n",
    "# random.shuffle(datasets_CC)\n",
    "\n",
    "# Find the smallest category\n",
    "min_category_size = min(len(datasets_AA), len(datasets_BB), len(datasets_CC))#, len(datasets_DD), len(datasets_EE))\n",
    "\n",
    "# Trim each category to the size of the smallest category\n",
    "datasets_AA = datasets_AA[:min_category_size]\n",
    "datasets_BB = datasets_BB[:min_category_size]\n",
    "datasets_CC = datasets_CC[:min_category_size]\n",
    "datasets_DD = datasets_DD[:min_category_size]\n",
    "datasets_EE = datasets_EE[:min_category_size]\n",
    "\n",
    "\n",
    "# Combine the data file names into categories\n",
    "data_categories = [datasets_AA, datasets_BB, datasets_CC]#, datasets_DD, datasets_EE]\n",
    "\n",
    "# Trim the data in each category to the minimum total length\n",
    "dataSets = trim_data_to_min_file_length(data_categories)\n",
    "\n",
    "# Define the neural network model / Last layer needs to match number of bins\n",
    "model_cg = Sequential([\n",
    "    GRU(units=16, activation='tanh', return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    Dropout(0.05),\n",
    "    GRU(units=8, activation='tanh', return_sequences=False, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    Dropout(0.05),\n",
    "    Dense(units=num_bins, activation='softmax')  \n",
    "    # GRU(units=128, activation='tanh', return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    # Dropout(0.2),\n",
    "    # GRU(units=64, activation='tanh', return_sequences=False, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    # Dropout(0.2),\n",
    "    # Dense(units=32, activation='relu'),\n",
    "    # Dense(units=3, activation='softmax') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model_cg.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=3)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='loss', patience=2, verbose=1, factor=0.5)\n",
    "model_checkpoint_acc = ModelCheckpoint('CG_Model.h5', save_best_only=True, monitor='accuracy', mode='max', verbose=1)\n",
    "\n",
    "print(dataSets[0][0], \"Loaded\")\n",
    "inputs, outputs = RNN_load_data(dataSets[0][0], timesteps, data_coeff)\n",
    "\n",
    "first_dataset_loaded = False\n",
    "for category in dataSets:\n",
    "    for dataSet in category:\n",
    "        if not first_dataset_loaded:\n",
    "            first_dataset_loaded = True\n",
    "            continue\n",
    "        print(dataSet, \"Loaded\")\n",
    "        inputData, outputData = RNN_load_data(dataSet, timesteps, data_coeff)\n",
    "        inputs = np.concatenate((inputs, inputData), axis=0)\n",
    "        outputs = np.concatenate((outputs, outputData), axis=0)\n",
    "print(\"Data Loading Finished\")\n",
    "print(\"Data Length:\", len(inputs))\n",
    "\n",
    "# Train the model\n",
    "model_cg.fit(inputs, outputs, epochs=25, batch_size=1024, callbacks=[early_stopping, lr_reduction, model_checkpoint_acc])\n",
    "\n",
    "# Save the model (Unnecessary because the ModelCheckpoint callback already saves it)\n",
    "# model_cg.save('CG_Model.h5')\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/coder/workspace/Goin/Fennec2023/Q3_Code/Q3_Trainer2.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://fennec1.jaedynchilton.com/home/coder/workspace/Goin/Fennec2023/Q3_Code/Q3_Trainer2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model_cg \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mCG_Model.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://fennec1.jaedynchilton.com/home/coder/workspace/Goin/Fennec2023/Q3_Code/Q3_Trainer2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Continue training the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://fennec1.jaedynchilton.com/home/coder/workspace/Goin/Fennec2023/Q3_Code/Q3_Trainer2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model_cg\u001b[39m.\u001b[39;49mfit(inputs, outputs, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2024\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stopping, lr_reduction, model_checkpoint_acc])\n\u001b[1;32m     <a href='vscode-notebook-cell://fennec1.jaedynchilton.com/home/coder/workspace/Goin/Fennec2023/Q3_Code/Q3_Trainer2.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Saved\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "os.chdir('/home/coder/workspace/Data/Becky_Data/')\n",
    "\n",
    "# Load the previously trained model\n",
    "model_cg = load_model('CG_Model.h5')\n",
    "\n",
    "# Continue training the model\n",
    "model_cg.fit(inputs, outputs, epochs=10, batch_size=2024, callbacks=[early_stopping, lr_reduction, model_checkpoint_acc])\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
